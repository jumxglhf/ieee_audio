\section{Introduction and Background}
The goal of the IEEE 2018 FEMH Voice Data Challenge was to develop an effective algorithmic approach to classifying voice samples as normal or pathological, and further subdivide the pathological samples into three types: vocal palsy, phonotrauma or neoplasm. As noted by the challenge organizers, voice disorders can severely affect quality of life, since humans primarily rely on speech for communication. At the same time, diagnoses of vocal disorders depend on larygeal endoscopy, which requires human expertise and specialized equipment. Thus there is a need for algorithms that could for example be used to triage cases to focus scarce resources.

The task of differentiating normal from pathological voice samples has been investigated in previous work, with a summary of previous approaches in~\cite{b9}. Among these approaches are artificial neural network with Mel-Frequency-Cepstral-Coefficients(MFCC), feature vectors from multiple frames that collectively represent the power spectrum of a sample~\cite{b5}. Others have used wavelet transform with support vector machines (SVMs)~\cite{b4}. Recently a deep neural network~\cite{b9} has been used by dividing the raw waveform into multiple segments and extracting normalized MFCC features from them.

A concern with some previous work is that they use a very old database of voice disorders from Massachusetts Eye and Ear Infirmary (MEEI), collected in 1994. It is not clear how well methods developed on this database might generalize to new data, for example because the recordings occurred in a well controlled environment. Indeed, recent previous work shows that learning algorithms can typically achieve a very high accuracy on data from this database. A contribution of the challenge organizers was to create a new dataset to see how well the MEEI results generalized to other cases. As the organizers note~\cite{}, the released dataset contains voice samples obtained from a voice clinic in the Far Eastern Memorial Hospital (FEMH). The training dataset includes 50 normal voice samples and 150 samples of common voice disorders, including vocal nodules, polyps, and cysts; laryngeal neoplasm; and unilateral vocal paralysis. The labels of the training dataset includes gender, age, whether the speaker is health or not, and the corresponding voice diseases. There are 400 testing samples without any labels for performance evaluation. All FEMH voice samples are a 3 second sustained vowel sound /a:/, which were recorded at a comfortable level of loudness, with a microphone-to-mouth distance of approximately $15-20$ cm, using a high-quality microphone (Model: SM58, SHURE) with a digital amplifier (Model: X2u, SHURE). X\% of the samples were male and the rest female. The sampling rate was 44100 Hz with a $16-$bit resolution, and data were saved in an uncompressed .wav format. Although we were allowed to use the MEEI database to train our models, we elected not to do so.

